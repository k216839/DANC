(DATN) PS D:\hkk1907\DATN\DANC> python .\mdmtn_mm.py     
2025-05-31 12:03:30.695466
Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz
Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz
Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz
Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz
Processing...
Done!
Data loaded!
Training... [--- running on cuda ---]
################################
#### SPARSITY inducing ... ####
################################
(10, 1, 5, 5) (10, np.int64(25))
(20, 10, 5, 5) (20, np.int64(250))
(50, 320) (50, np.int64(320))
(10, 50) (10, np.int64(50))
(10, 50) (10, np.int64(50))
(20, 1, 5, 5) (20, np.int64(25))
(50, 2880) (50, np.int64(2880))
(20, 1, 5, 5) (20, np.int64(25))
(50, 2880) (50, np.int64(2880))
0
std at layer  0  =  1.2392634
std at layer  0  =  1.0000002 mean =  -0.013782987
finish at layer 0
1
std at layer  1  =  1.2017536
std at layer  1  =  0.99999994 mean =  0.18620929
finish at layer 1
2
std at layer  2  =  1.1010132
std at layer  2  =  0.9999999 mean =  0.10202562
finish at layer 2
3
std at layer  3  =  0.6078943
std at layer  3  =  1.0 mean =  -0.75558937
finish at layer 3
4
std at layer  4  =  0.8809622
std at layer  4  =  1.0 mean =  -0.32923684
finish at layer 4
5
std at layer  5  =  1.1608772
std at layer  5  =  1.0 mean =  -0.014711211
finish at layer 5
6
std at layer  6  =  0.8542304
std at layer  6  =  1.0 mean =  0.19522268
finish at layer 6
7
std at layer  7  =  1.1357588
std at layer  7  =  1.0000004 mean =  0.039767306
finish at layer 7
8
std at layer  8  =  0.98426735
finish at layer 8
LSUV init done!
-------------------------------------
------ Algorithm Iteration 1/3 ------
-------------------------------------
######################
#### EPOCH No 1/3 ####
######################
[BATCH (50) (26%)]      Loss: 0.268223
[BATCH (100) (51%)]     Loss: 0.197486
[BATCH (150) (77%)]     Loss: 0.204820
Applying GrOWL ....
Done !

Validation set: Average Accuracy: (88.24%)

Sparsity Ratio:  24.81834329081221
Best global performance (Accuracy)!
Accuracy Task 1: 87.9100%
Accuracy Task 2: 88.5600%
######################
#### EPOCH No 2/3 ####
######################
[BATCH (50) (26%)]      Loss: 0.147726
[BATCH (100) (51%)]     Loss: 0.150289
[BATCH (150) (77%)]     Loss: 0.155123
Applying GrOWL ....
Done !

Validation set: Average Accuracy: 91.35%    (Best: 91.35%)

Sparsity Ratio:  26.255449701275634
Best global performance (Accuracy)!
Accuracy Task 1: 93.4100%
Accuracy Task 2: 89.2800%
######################
#### EPOCH No 3/3 ####
######################
[BATCH (50) (26%)]      Loss: 0.111269
[BATCH (100) (51%)]     Loss: 0.116636
[BATCH (150) (77%)]     Loss: 0.156375
Applying GrOWL ....
Done !

Validation set: Average Accuracy: 93.49%    (Best: 93.49%)

Sparsity Ratio:  26.320038753431295
Best global performance (Accuracy)!
Accuracy Task 1: 93.6100%
Accuracy Task 2: 93.3800%
Learning rate used:  0.0025
Penalty coefficient (mu) used:  2.5e-08
-------------------------------------
------ Algorithm Iteration 2/3 ------
-------------------------------------
######################
#### EPOCH No 1/3 ####
######################
[BATCH (50) (26%)]      Loss: 0.085848
[BATCH (100) (51%)]     Loss: 0.113076
[BATCH (150) (77%)]     Loss: 0.088631
Applying GrOWL ....
Done !

Validation set: Average Accuracy: 91.85%    (Best: 93.49%)

Sparsity Ratio:  52.41401582431778
######################
#### EPOCH No 2/3 ####
######################
[BATCH (50) (26%)]      Loss: 0.097322
[BATCH (100) (51%)]     Loss: 0.077148
[BATCH (150) (77%)]     Loss: 0.077783
Applying GrOWL ....
Done !

Validation set: Average Accuracy: 90.26%    (Best: 93.49%)

Sparsity Ratio:  52.41401582431778
######################
#### EPOCH No 3/3 ####
######################
[BATCH (50) (26%)]      Loss: 0.089606
[BATCH (100) (51%)]     Loss: 0.082031
[BATCH (150) (77%)]     Loss: 0.099449
Applying GrOWL ....
Done !

Validation set: Average Accuracy: 89.65%    (Best: 93.49%)

Sparsity Ratio:  52.46245761343452
Learning rate used:  0.00125
Penalty coefficient (mu) used:  5e-08
-------------------------------------
------ Algorithm Iteration 3/3 ------
-------------------------------------
######################
#### EPOCH No 1/3 ####
######################
[BATCH (50) (26%)]      Loss: 0.083272
[BATCH (100) (51%)]     Loss: 0.069013
[BATCH (150) (77%)]     Loss: 0.084637
Applying GrOWL ....
Done !

Validation set: Average Accuracy: 74.26%    (Best: 93.49%)

Sparsity Ratio:  78.66946552559341
######################
#### EPOCH No 2/3 ####
######################
[BATCH (50) (26%)]      Loss: 0.088865
[BATCH (100) (51%)]     Loss: 0.061620
[BATCH (150) (77%)]     Loss: 0.071621
Applying GrOWL ....
Done !

Validation set: Average Accuracy: 80.18%    (Best: 93.49%)

Sparsity Ratio:  78.63717099951558
######################
#### EPOCH No 3/3 ####
######################
[BATCH (50) (26%)]      Loss: 0.070459
[BATCH (100) (51%)]     Loss: 0.076591
[BATCH (150) (77%)]     Loss: 0.062612
Applying GrOWL ....
Done !

Validation set: Average Accuracy: 82.24%    (Best: 93.49%)

Sparsity Ratio:  78.68561278863233
Learning rate used:  0.000625
Penalty coefficient (mu) used:  1e-07
 ####### Training Results ####### 
Sparsity Rate:  26.320038753431295
Compression Rate:  1.3569237510955303
Parameter Sharing:  0.9997808939526731
 ################################
Name:  Shared_block.0.weight
Insignificant Neurons: 0/1 (0.0)
====================================
Name:  Shared_block.3.weight
Insignificant Neurons: 0/10 (0.0)
====================================
Name:  Shared_block.7.weight
Insignificant Neurons: 86/320 (26.875)
====================================
Name:  task_blocks.0.0.weight
Insignificant Neurons: 0/50 (0.0)
====================================
Name:  task_blocks.1.0.weight
Insignificant Neurons: 0/50 (0.0)
====================================
Name:  monitors.0.0.weight
Insignificant Neurons: 0/1 (0.0)
====================================
Name:  monitors.0.4.weight
Insignificant Neurons: 773/2880 (26.84027777777778)
====================================
Name:  monitors.1.0.weight
Insignificant Neurons: 0/1 (0.0)
====================================
Name:  monitors.1.4.weight
Insignificant Neurons: 771/2880 (26.770833333333332)
====================================
Sparsity Ratio:  26.320038753431295
Computing similarity matrices . . .
C:\Users\admin\anaconda3\envs\DATN\lib\site-packages\sklearn\cluster\_affinity_propagation.py:140: ConvergenceWarning: Affinity propagation did not converge, this model may return degenerate cluster centers and labels.
  warnings.warn(
C:\Users\admin\anaconda3\envs\DATN\lib\site-packages\sklearn\cluster\_affinity_propagation.py:140: ConvergenceWarning: Affinity propagation did not converge, this model may return degenerate cluster centers and labels.
  warnings.warn(
C:\Users\admin\anaconda3\envs\DATN\lib\site-packages\sklearn\cluster\_affinity_propagation.py:140: ConvergenceWarning: Affinity propagation did not converge, this model may return degenerate cluster centers and labels.
  warnings.warn(
Done !
2025-05-31 12:16:59.462361
###############################
#### RETRAINING started ! ####
###############################
-------------------------------------
------ Algorithm Iteration 1/10 ------
-------------------------------------
######################
#### EPOCH No 1/3 ####
######################
[BATCH (50) (26%)]      Loss: 0.108259
[BATCH (100) (51%)]     Loss: 0.103773
[BATCH (150) (77%)]     Loss: 0.100756
Forcing parameter sharing....
Done !

Validation set: Average Accuracy: (95.39%)

Best global performance (Accuracy)!
Accuracy Task 1: 95.8700%
Accuracy Task 2: 94.9000%
######################
#### EPOCH No 2/3 ####
######################
[BATCH (50) (26%)]      Loss: 0.115850
[BATCH (100) (51%)]     Loss: 0.124385
[BATCH (150) (77%)]     Loss: 0.125309
Forcing parameter sharing....
Done !

Validation set: Average Accuracy: 95.64%    (Best: 95.64%)

Best global performance (Accuracy)!
Accuracy Task 1: 96.1000%
Accuracy Task 2: 95.1800%
######################
#### EPOCH No 3/3 ####
######################
[BATCH (50) (26%)]      Loss: 0.127439
[BATCH (100) (51%)]     Loss: 0.122704
[BATCH (150) (77%)]     Loss: 0.122919
Forcing parameter sharing....
Done !

Validation set: Average Accuracy: 96.18%    (Best: 96.18%)

Best global performance (Accuracy)!
Accuracy Task 1: 96.4700%
Accuracy Task 2: 95.8900%
Learning rate used:  0.0025
Penalty coefficient (mu) used:  2.5e-08
-------------------------------------
------ Algorithm Iteration 2/10 ------
-------------------------------------
######################
#### EPOCH No 1/3 ####
######################
[BATCH (50) (26%)]      Loss: 0.102299
[BATCH (100) (51%)]     Loss: 0.107177
[BATCH (150) (77%)]     Loss: 0.109070
Forcing parameter sharing....
Done !

Validation set: Average Accuracy: 96.53%    (Best: 96.53%)

Best global performance (Accuracy)!
Accuracy Task 1: 96.6500%
Accuracy Task 2: 96.4000%
######################
#### EPOCH No 2/3 ####
######################
[BATCH (50) (26%)]      Loss: 0.110425
[BATCH (100) (51%)]     Loss: 0.108811
[BATCH (150) (77%)]     Loss: 0.108346
Forcing parameter sharing....
Done !

Validation set: Average Accuracy: 96.57%    (Best: 96.57%)

Best global performance (Accuracy)!
Accuracy Task 1: 96.7300%
Accuracy Task 2: 96.4100%
######################
#### EPOCH No 3/3 ####
######################
[BATCH (50) (26%)]      Loss: 0.107984
[BATCH (100) (51%)]     Loss: 0.115411
[BATCH (150) (77%)]     Loss: 0.119577
Forcing parameter sharing....
Done !

Validation set: Average Accuracy: 96.46%    (Best: 96.57%)

Learning rate used:  0.00125
Penalty coefficient (mu) used:  5e-08
-------------------------------------
------ Algorithm Iteration 3/10 ------
-------------------------------------
######################
#### EPOCH No 1/3 ####
######################
[BATCH (50) (26%)]      Loss: 0.105753
[BATCH (100) (51%)]     Loss: 0.108999
[BATCH (150) (77%)]     Loss: 0.107787
Forcing parameter sharing....
Done !

Validation set: Average Accuracy: 96.64%    (Best: 96.64%)

Best global performance (Accuracy)!
Accuracy Task 1: 96.6600%
Accuracy Task 2: 96.6100%
######################
#### EPOCH No 2/3 ####
######################
[BATCH (50) (26%)]      Loss: 0.106319
[BATCH (100) (51%)]     Loss: 0.109260
[BATCH (150) (77%)]     Loss: 0.117193
Forcing parameter sharing....
Done !

Validation set: Average Accuracy: 96.54%    (Best: 96.64%)

######################
#### EPOCH No 3/3 ####
######################
[BATCH (50) (26%)]      Loss: 0.109340
[BATCH (100) (51%)]     Loss: 0.106489
[BATCH (150) (77%)]     Loss: 0.115841
Forcing parameter sharing....
Done !

Validation set: Average Accuracy: 96.69%    (Best: 96.69%)

Best global performance (Accuracy)!
Accuracy Task 1: 96.7400%
Accuracy Task 2: 96.6400%
Learning rate used:  0.000625
Penalty coefficient (mu) used:  1e-07
-------------------------------------
------ Algorithm Iteration 4/10 ------
-------------------------------------
######################
#### EPOCH No 1/3 ####
######################
[BATCH (50) (26%)]      Loss: 0.105853
[BATCH (100) (51%)]     Loss: 0.105582
[BATCH (150) (77%)]     Loss: 0.108164
Forcing parameter sharing....
Done !

Validation set: Average Accuracy: 96.72%    (Best: 96.72%)

Best global performance (Accuracy)!
Accuracy Task 1: 96.7100%
Accuracy Task 2: 96.7300%
######################
#### EPOCH No 2/3 ####
######################
[BATCH (50) (26%)]      Loss: 0.106971
[BATCH (100) (51%)]     Loss: 0.106550
[BATCH (150) (77%)]     Loss: 0.105731
Forcing parameter sharing....
Done !

Validation set: Average Accuracy: 96.70%    (Best: 96.72%)

######################
#### EPOCH No 3/3 ####
######################
[BATCH (50) (26%)]      Loss: 0.105428
[BATCH (100) (51%)]     Loss: 0.107118
[BATCH (150) (77%)]     Loss: 0.106393
Forcing parameter sharing....
Done !

Validation set: Average Accuracy: 96.64%    (Best: 96.72%)

Learning rate used:  0.0003125
Penalty coefficient (mu) used:  2e-07
-------------------------------------
------ Algorithm Iteration 5/10 ------
-------------------------------------
######################
#### EPOCH No 1/3 ####
######################
[BATCH (50) (26%)]      Loss: 0.105474
[BATCH (100) (51%)]     Loss: 0.105658
[BATCH (150) (77%)]     Loss: 0.105816
Forcing parameter sharing....
Done !

Validation set: Average Accuracy: 96.68%    (Best: 96.72%)

######################
#### EPOCH No 2/3 ####
######################
[BATCH (50) (26%)]      Loss: 0.108347
[BATCH (100) (51%)]     Loss: 0.105829
[BATCH (150) (77%)]     Loss: 0.106845
Forcing parameter sharing....
Done !

Validation set: Average Accuracy: 96.66%    (Best: 96.72%)

######################
#### EPOCH No 3/3 ####
######################
[BATCH (50) (26%)]      Loss: 0.105303
[BATCH (100) (51%)]     Loss: 0.105520
[BATCH (150) (77%)]     Loss: 0.108369
Forcing parameter sharing....
Done !

Validation set: Average Accuracy: 96.71%    (Best: 96.72%)

Learning rate used:  0.00015625
Penalty coefficient (mu) used:  4e-07
-------------------------------------
------ Algorithm Iteration 6/10 ------
-------------------------------------
######################
#### EPOCH No 1/3 ####
######################
[BATCH (50) (26%)]      Loss: 0.105944
[BATCH (100) (51%)]     Loss: 0.105943
[BATCH (150) (77%)]     Loss: 0.106468
Forcing parameter sharing....
Done !

Validation set: Average Accuracy: 96.68%    (Best: 96.72%)

######################
#### EPOCH No 2/3 ####
######################
[BATCH (50) (26%)]      Loss: 0.109143
[BATCH (100) (51%)]     Loss: 0.106067
[BATCH (150) (77%)]     Loss: 0.105914
Forcing parameter sharing....
Done !

Validation set: Average Accuracy: 96.65%    (Best: 96.72%)

######################
#### EPOCH No 3/3 ####
######################
[BATCH (50) (26%)]      Loss: 0.106678
[BATCH (100) (51%)]     Loss: 0.106279
[BATCH (150) (77%)]     Loss: 0.106946
Forcing parameter sharing....
Done !

Validation set: Average Accuracy: 96.66%    (Best: 96.72%)

Learning rate used:  7.8125e-05
Penalty coefficient (mu) used:  8e-07
-------------------------------------
------ Algorithm Iteration 7/10 ------
-------------------------------------
######################
#### EPOCH No 1/3 ####
######################
[BATCH (50) (26%)]      Loss: 0.107399
[BATCH (100) (51%)]     Loss: 0.106509
[BATCH (150) (77%)]     Loss: 0.106783
Forcing parameter sharing....
Done !

Validation set: Average Accuracy: 96.65%    (Best: 96.72%)

######################
#### EPOCH No 2/3 ####
######################
[BATCH (50) (26%)]      Loss: 0.106578
[BATCH (100) (51%)]     Loss: 0.107369
[BATCH (150) (77%)]     Loss: 0.106942
Forcing parameter sharing....
Done !

Validation set: Average Accuracy: 96.67%    (Best: 96.72%)

######################
#### EPOCH No 3/3 ####
######################
[BATCH (50) (26%)]      Loss: 0.106677
[BATCH (100) (51%)]     Loss: 0.106834
[BATCH (150) (77%)]     Loss: 0.107035
Forcing parameter sharing....
Done !

Validation set: Average Accuracy: 96.68%    (Best: 96.72%)

Learning rate used:  3.90625e-05
Penalty coefficient (mu) used:  1.6e-06
-------------------------------------
------ Algorithm Iteration 8/10 ------
-------------------------------------
######################
#### EPOCH No 1/3 ####
######################
[BATCH (50) (26%)]      Loss: 0.108660
[BATCH (100) (51%)]     Loss: 0.108160
[BATCH (150) (77%)]     Loss: 0.108142
Forcing parameter sharing....
Done !

Validation set: Average Accuracy: 96.65%    (Best: 96.72%)

######################
#### EPOCH No 2/3 ####
######################
[BATCH (50) (26%)]      Loss: 0.108909
[BATCH (100) (51%)]     Loss: 0.108535
[BATCH (150) (77%)]     Loss: 0.108613
Forcing parameter sharing....
Done !

Validation set: Average Accuracy: 96.66%    (Best: 96.72%)

######################
#### EPOCH No 3/3 ####
######################
[BATCH (50) (26%)]      Loss: 0.108084
[BATCH (100) (51%)]     Loss: 0.108260
[BATCH (150) (77%)]     Loss: 0.108275
Forcing parameter sharing....
Done !

Validation set: Average Accuracy: 96.67%    (Best: 96.72%)

Learning rate used:  1.953125e-05
Penalty coefficient (mu) used:  3.2e-06
-------------------------------------
------ Algorithm Iteration 9/10 ------
-------------------------------------
######################
#### EPOCH No 1/3 ####
######################
[BATCH (50) (26%)]      Loss: 0.111423
[BATCH (100) (51%)]     Loss: 0.111272
[BATCH (150) (77%)]     Loss: 0.110692
Forcing parameter sharing....
Done !

Validation set: Average Accuracy: 96.67%    (Best: 96.72%)

######################
#### EPOCH No 2/3 ####
######################
[BATCH (50) (26%)]      Loss: 0.110975
[BATCH (100) (51%)]     Loss: 0.110916
[BATCH (150) (77%)]     Loss: 0.110893
Forcing parameter sharing....
Done !

Validation set: Average Accuracy: 96.65%    (Best: 96.72%)

######################
#### EPOCH No 3/3 ####
######################
[BATCH (50) (26%)]      Loss: 0.111267
[BATCH (100) (51%)]     Loss: 0.110925
[BATCH (150) (77%)]     Loss: 0.111131
Forcing parameter sharing....
Done !

Validation set: Average Accuracy: 96.66%    (Best: 96.72%)

Learning rate used:  9.765625e-06
Penalty coefficient (mu) used:  6.4e-06
-------------------------------------
------ Algorithm Iteration 10/10 ------
-------------------------------------
######################
#### EPOCH No 1/3 ####
######################
[BATCH (50) (26%)]      Loss: 0.116674
[BATCH (100) (51%)]     Loss: 0.116498
[BATCH (150) (77%)]     Loss: 0.116578
Forcing parameter sharing....
Done !

Validation set: Average Accuracy: 96.65%    (Best: 96.72%)

######################
#### EPOCH No 2/3 ####
######################
[BATCH (50) (26%)]      Loss: 0.117122
[BATCH (100) (51%)]     Loss: 0.116474
[BATCH (150) (77%)]     Loss: 0.116870
Forcing parameter sharing....
Done !

Validation set: Average Accuracy: 96.66%    (Best: 96.72%)

######################
#### EPOCH No 3/3 ####
######################
[BATCH (50) (26%)]      Loss: 0.116448
[BATCH (100) (51%)]     Loss: 0.116518
[BATCH (150) (77%)]     Loss: 0.116549
Forcing parameter sharing....
Done !

Validation set: Average Accuracy: 96.65%    (Best: 96.72%)

Learning rate used:  4.8828125e-06
Penalty coefficient (mu) used:  1.28e-05
 ####### Training Results ####### 
Sparsity Rate:  26.320038753431295
Compression Rate:  2.0684702738810956
Parameter Sharing:  1.5240480961923848
 ################################

Computation time for RETRAINING: 35.591227666536966 minutes
2025-05-31 12:52:34.937022
Training completed !

Computation time: 49.070692594846086 minutes
2025-05-31 12:52:34.937022
Testing ...
logs/MDMTN_MM_logs/MDMTN_model_MM_onek/model000.pth
Model loaded !

Test set: Average Accuracy: (96.35%)

Accuracy Task 1: 96.7100%
Accuracy Task 2: 95.9900%